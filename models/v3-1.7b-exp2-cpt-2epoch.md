# LLM-jp v3 1.7B experiment 2

# Model specs

|Spec name|Value|
|:---|:---|
|Base model structure|Llama-2|
|Number of layers|24|
|Number of embedding units(hidden size)|2048|
|Number of FFN hidden units(intermediate size)|7168|
|Number of attention heads|16|
|Vocabulary size|99,487|
|Tokenizer|[llm-jp-tokenizer-100k.ver3.0b1.model](https://github.com/llm-jp/llm-jp-tokenizer/blob/870a27ce6872e105e4b76cdf2e68c8b7ebfc6a37/models/ver3.0/llm-jp-tokenizer-100k.ver3.0b1.model)|
|Maximum sequence length|4096|
|Positional embedding|RoPE|

# Dataset specs
|Spec name|Value|
|:---|:---|
|Dataset|LLM-jp v3.1|
|Number of tokens|2,072,488,058,295|

# Training specs

|Spec name| Value                                                                                              |
|:---|:---------------------------------------------------------------------------------------------------|
|Implementation| [Megatron-LM](https://github.com/llm-jp/Megatron-LM/tree/402e6ff1937176db4e1e3dedfefaa15f97204d84) |
|Optimizer| AdamW                                                                                              |
|Initial learning rate| 3e-5                                                                                               |
|Final learning rate| 3e-5                                                                                               |
|Beta1| 0.9                                                                                                |
|Beta2| 0.95                                                                                               |
|Epsilon| 1e-8                                                                                               |
|Weight decay factor| 0.1                                                                                                |
|Warmup strategy| Linear                                                                                             |
|Warmup steps| 0                                                                                                  |
|Learning rate scheduling strategy| Constant                                                                                           |
|Learning rate scheduling steps| 988,240                                                                                            |
|Total training steps| 988,240                                                                                            |
|Global batch size| 512                                                                                                |
|Dropout rate| 0.0                                                                                                |
|Floating point precisions| BF16                                                                                               |
|Additional randomness/approximation| Flash Attention                                                                                    |
|Z loss| 1e-4                                                                                               |
|Embedding scale| ❌                                                                                                  |
|Tensor parallel size| 1                                                                                                  |
|Pipeline parallel size| 1                                                                                                  |

# Environmental specs

|Spec name| Value                    |
|:---|:-------------------------|
|Name| Sakura Internet          |
|Number of Nodes| 8                        |
|Processor| Intel Xeon Platinum 8480 |
|Number of CPUs per instance| 2                        |
|RAM| 2.0TB                    |
|Accelerators| NVIDIA H100 80GB         |
|Number of accelerators per instance| 8                        |
|Intra Node Communication | NVLink                   |


# Comments
v3 コーパス２周目の効果検証

v3 コーパスは 2.1T トークンだが，英語は１周分であるのに対し，日本語は既に２周分使っていることに注意．
そのため，v3 コーパスを２周すると英語は２周，日本語は４周することになる．
